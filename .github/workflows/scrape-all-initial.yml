name: STEP 1 - Download TUTTO e genera JSON per anni

on:
  workflow_dispatch:
    inputs:
      max_pages:
        description: 'Numero massimo pagine (0 = tutte le ~5591). Ignorato se usi start_page/end_page'
        required: false
        default: '0'
      start_page:
        description: 'Pagina iniziale (opzionale, per scaricare chunk specifico)'
        required: false
        default: ''
      end_page:
        description: 'Pagina finale (opzionale, richiede start_page)'
        required: false
        default: ''

# Previene conflitti: solo un workflow scraping alla volta
concurrency:
  group: scraping-workflows
  cancel-in-progress: false  # Nuovi workflow aspettano invece di cancellare quelli in corso

jobs:
  scrape-all:
    runs-on: ubuntu-latest
    timeout-minutes: 600  # 10 ore max per download completo
    permissions:
      contents: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch || github.ref }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install Chrome
        run: |
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt-get install -y ./google-chrome-stable_current_amd64.deb
          rm google-chrome-stable_current_amd64.deb

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: STEP 1.1 - Download TUTTI gli HTML (CIVILE - QUINTA)
        run: |
          START_PAGE="${{ github.event.inputs.start_page }}"
          END_PAGE="${{ github.event.inputs.end_page }}"
          MAX_PAGES="${{ github.event.inputs.max_pages || '0' }}"

          # Caso 1: Range specifico (start_page e end_page)
          if [ -n "$START_PAGE" ] && [ -n "$END_PAGE" ]; then
            echo "ðŸ“¥ Download range: pagina $START_PAGE â†’ $END_PAGE"
            NUM_PAGES=$((END_PAGE - START_PAGE + 1))
            echo "â±ï¸  Tempo stimato: ~$((NUM_PAGES * 2 / 60)) minuti (delay 2.0s/pagina)"
            python3 scraper/scripts/1_download_html_range.py \
              --start $START_PAGE \
              --end $END_PAGE \
              --output scraper/data/html

          # Caso 2: Tutte le pagine o max_pages
          elif [ "$MAX_PAGES" = "0" ]; then
            echo "ðŸ“¥ Download TUTTE le pagine (~5591 totali)..."
            echo "â±ï¸  Tempo stimato: ~6-7 ore (delay 2.0s/pagina)"
            python3 scraper/scripts/1_download_html.py \
              --pages 99999 \
              --output scraper/data/html
          else
            echo "ðŸ“¥ Download max $MAX_PAGES pagine..."
            echo "â±ï¸  Tempo stimato: ~$((MAX_PAGES * 2 / 60)) minuti (delay 2.0s/pagina)"
            python3 scraper/scripts/1_download_html.py \
              --pages $MAX_PAGES \
              --output scraper/data/html
          fi

      - name: STEP 1.2 - Parse HTML e genera TUTTI i JSON per anno
        run: |
          echo "ðŸ” Parsing HTML â†’ JSON per TUTTI gli anni..."
          echo "ðŸ“Š Strategia efficiente: UN download, TUTTI i JSON!"
          python3 scraper/scripts/2_parse_html_to_json.py \
            --html-dir scraper/data/html \
            --all-years \
            --delete-html

      - name: Check for changes
        id: check_changes
        run: |
          git status
          if [ -n "$(git status --porcelain)" ]; then
            echo "changes=true" >> $GITHUB_OUTPUT
            echo "âœ… Changes detected"
          else
            echo "changes=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸  No changes to commit"
          fi

      - name: Commit and push changes
        if: steps.check_changes.outputs.changes == 'true'
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          # Add tutti i JSON generati
          git add metadata/metadata_cassazione_*.json

          # Commit
          git commit -m "STEP 1 completato: Tutti i JSON per anno generati - $(date +'%Y-%m-%d %H:%M UTC')" \
                     -m "- Download completo HTML CIVILE - QUINTA" \
                     -m "- JSON separati per ogni anno: 2020, 2021, 2022, 2023, 2024, 2025" \
                     -m "- HTML cancellati dopo parsing" \
                     -m "- Prossimo step: STEP 2 per scaricare PDF anno per anno"

          # Pull con rebase prima di pushare (evita conflitti se il branch Ã¨ stato aggiornato)
          echo "ðŸ”„ Sincronizzazione con remote..."
          git pull --rebase origin ${{ github.ref_name }} || echo "âš ï¸  Nessun conflitto da risolvere"

          # Push
          echo "ðŸ“¤ Push al remote..."
          git push

      - name: Summary
        if: always()
        run: |
          echo "## ðŸ“Š STEP 1 - Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### JSON Generati:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for json_file in metadata/metadata_cassazione_*.json; do
            if [ -f "$json_file" ]; then
              YEAR=$(basename "$json_file" | sed 's/metadata_cassazione_//;s/.json//')
              TOTAL=$(python3 -c "import json; data=json.load(open('$json_file')); print(data['metadata']['total_sentences'])")
              echo "- **$YEAR**: $TOTAL sentenze" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "âœ… STEP 1 completato!" >> $GITHUB_STEP_SUMMARY
          echo "ðŸ“ Prossimo: usa workflow 'STEP 2' per scaricare PDF di un anno specifico" >> $GITHUB_STEP_SUMMARY
